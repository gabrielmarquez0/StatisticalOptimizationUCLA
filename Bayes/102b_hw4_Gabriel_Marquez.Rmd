---
title: "Stats 102B Homework 4"
author: "Gabriel Marquez"
footer: "Homework Assignment Copyright Miles Chen. Do not post or distribute without permission."
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Naive Bayes Classifier for Iris data

Task: Write a function that performs Naive Bayes classification for the iris data. The function will output probabiity estimates of the species for a test case.

The function will accept three inputs: a row matrix for the x values of the test case, a matrix of x values for the training data, and a vector of class labels for the training data.

The function will create the probability estimates based on the training data it has been provided.

Within the function use a Gaussian model and estimate the mean and standard deviation of the Gaussian populations based on the training data provided. (Hint: You have 24 parameters to estimate: the mean and standard deviation of each of the 4 variables for each of the three species. With the naive assumption, you do not have to estimate any covariances.)


```{r, error = TRUE}
library(dplyr)
iris_nb <- function(testx, trainx, trainy){
  # Write your code here    
  rownames(trainx) <- seq(length = nrow(trainx))
  

  x_set <- trainx[trainy == "setosa", ]
  x_ver <- trainx[trainy == "versicolor", ]
  x_vir <- trainx[trainy == "virginica", ]
  
  x_bar <- colMeans(trainx)
  x_sd <- apply(trainx, 2, sd)
  
  x_bar_set <- colMeans(x_set)
  x_sd_set <- apply(x_set, 2, sd)
  x_bar_ver <- colMeans(x_ver)
  x_sd_ver <- apply(x_ver, 2, sd)
  x_bar_vir <- colMeans(x_vir)
  x_sd_vir <- apply(x_vir, 2, sd)
  
  N_set <- dim(x_set)[1]
  N_ver <- dim(x_ver)[1]
  N_vir <- dim(x_vir)[1]
  
  N_tot <- (N_set + N_ver + N_vir)
  
  like_set <- prod(1-pnorm(abs((testx - x_bar_set) * (1/x_sd_set)))) * (N_set / N_tot)
  like_ver <- prod(1-pnorm(abs((testx - x_bar_ver) * (1/x_sd_ver)))) * (N_ver / N_tot)
  like_vir <- prod(1-pnorm(abs((testx - x_bar_vir) * (1/x_sd_vir)))) * (N_vir / N_tot)
  
  marginal <- like_set + like_ver + like_vir
  
  values <- c(like_set, like_ver, like_vir)/marginal
  names(values) <- c("setosa", "versicolor", "virginica")
  
  return(values)
}

```


```c
### output should be a named vector that looks something like this:     
## [these numbers are completely made up btw]     
    setosa versicolor  virginica     
 0.9518386  0.0255936  0.0225678     
```


#### Testing it out

```{r, error = TRUE}
set.seed(1)
training_rows <- sort(c(sample(1:50, 40), sample(51:100, 40), sample(101:150, 40)))
training_x <- as.matrix(iris[training_rows, 1:4])
training_y <- iris[training_rows, 5]


# test cases     
test_case_a <- as.matrix(iris[24, 1:4]) # true class setosa       
test_case_b <- as.matrix(iris[73, 1:4]) # true class versicolor     
test_case_c <- as.matrix(iris[124, 1:4]) # true class virginica   

# class predictions of test cases     
iris_nb(test_case_a, training_x, training_y)
iris_nb(test_case_b, training_x, training_y)
iris_nb(test_case_c, training_x, training_y)
```

```{r, error = TRUE}
# should work and produce slightly different estimates based on new training data     
set.seed(10)
training_rows2 <- sort(c(sample(1:50, 25), sample(51:100, 25), sample(101:150, 25)))
training_x2 <- as.matrix(iris[training_rows2, 1:4])
training_y2 <- iris[training_rows2, 5]

iris_nb(test_case_a, training_x2, training_y2)
iris_nb(test_case_b, training_x2, training_y2)
iris_nb(test_case_c, training_x2, training_y2)
```

### Naive Bayes with R

While instructive and education (I hope) to write your own NaiveBayes function, in practical settings, I recommend using the production ready code from some time-tested packages.

I've included some code for using the `naiveBayes()` function that is part of the `e1071` package. No need to modify anything. The results prediced by `naiveBayes()` should match the results from the function you wrote.

```{r}
# code provided. no need to edit. These results should match your results above.     
library(e1071)
nb_model1 <- naiveBayes(training_x, training_y)
predict(nb_model1, newdata = test_case_a, type = 'raw')
predict(nb_model1, newdata = test_case_b, type = 'raw')
predict(nb_model1, newdata = test_case_c, type = 'raw')

nb_model2 <- naiveBayes(training_x2, training_y2)
predict(nb_model2, newdata = test_case_a, type = 'raw')
predict(nb_model2, newdata = test_case_b, type = 'raw')
predict(nb_model2, newdata = test_case_c, type = 'raw')
```


# K-nearest neighbors Classifier for the Iris data

Task: Write a classifier using the K-nearest neighbors algorithm for the iris data set.

First write a function that will calculate the euclidean distance from a vector A (in 4-dimensional space) to another vector B (also in 4-dimensional space).

Use that function to find the k nearest neighbors to then make a classification.

The function will accept four inputs: a row matrix for the x values of the test case, a matrix of x values for the training data, a vector of class labels for the training data, and the k parameter.

The function will return a single label.


```{r, error = TRUE}
distance <- function(a, b){
  # Write your code here  
  sum((a-b)^2)^.5
}

iris_knn <- function(testx, trainx, trainy, k){
  # Write your code here     
  rownames(trainx) <- seq(length = nrow(trainx))

  neighbors <- sort(apply(trainx, 1, distance, b = testx))[1:k]

  kinds <- summary(trainy[as.integer(names(neighbors))])

  sample(names(kinds[kinds == max(kinds)]), 1)
}
```


```{r, error = TRUE}
iris_knn(test_case_a, training_x, training_y, 5)
iris_knn(test_case_b, training_x, training_y, 5) # will incorrectly label as virginica with this training data     
iris_knn(test_case_c, training_x, training_y, 5)

iris_knn(test_case_a, training_x2, training_y2, 5)
iris_knn(test_case_b, training_x2, training_y2, 5)
iris_knn(test_case_c, training_x2, training_y2, 5) # will incorrectly label as versicolor with this training data     
#For some reason, my function and the package function both correctly labeled as virginica.

```


### KNN with R

Again, if you plan on using KNN in real-life, use a function from a package.

I've included some code for using the `knn()` function that is part of the `class` package. No need to modify anything. The results prediced by `knn()` should match the results from the function you wrote, including the misclassification of some of the test cases based on the training data.

```{r}
library(class)
knn(train = training_x, cl = training_y, test = test_case_a, k = 5)
knn(train = training_x, cl = training_y, test = test_case_b, k = 5) # will incorrectly label as virginica with this training data     
knn(train = training_x, cl = training_y, test = test_case_c, k = 5)

knn(train = training_x2, cl = training_y2, test = test_case_a, k = 5)
knn(train = training_x2, cl = training_y2, test = test_case_b, k = 5)
knn(train = training_x2, cl = training_y2, test = test_case_c, k = 5) # will incorrectly label as versicolor with this training data     
```


## SVM 

Manual implementation of SVM is a bit of a pain (quadratic programming is hard), and I will not include it in the hw.

For the interested student, I refer them to the code from book's companion github repository: https://github.com/sdrogers/fcmlcode/blob/master/R/chapter5/svmhard.R and this post on stackexchange: https://stats.stackexchange.com/questions/179900/optimizing-a-support-vector-machine-with-quadratic-programming

Instead, I will use an example of a mixture model that can be separated via SVM.

The mixture model comes from the excellent (but advanced) textbook, *The Elements of Statistical Learning*, which is made to be freely available by the authors at: https://web.stanford.edu/~hastie/ElemStatLearn/

```{r}
library(ElemStatLearn)
data(mixture.example)
df <- data.frame(mixture.example$x, y = as.factor(mixture.example$y)) # turn the data into a dataframe     
plot(df$X1,df$X2, col = df$y, pch = 19) # create a plot of the mixture     
```


We will use the `svm()` function available in package `e1071`. 

Read the documentation on the function `svm()`.

For the following models, we will use a **radial-basis function**, which is equivalent to using a Gaussian Kernel function. (The Gaussian Kernel function projects the 2-dimensional data into infinite dimensional space and takes the inner product of these infinite dimensional vectors. It doesn't actually do this, but the resulting inner product can be found and used to draw a decision boundary.)

The svm function allows for multiple arguments, but we will focus on the effect of the arguments for gamma and cost.

I have created 9 classification models using SVM and different values of gamma and cost.

Pay attention to the values of `gamma` and `cost`. At the very end comment on the effect of each parameter on the resulting model.

```{r}
library(e1071)
model <- svm(y ~ . , data = df, scale = FALSE, kernel = "radial", gamma = 1, cost = 1)
```

```{r, echo = FALSE}
# Credit to this tutorial by James Le https://www.datacamp.com/community/tutorials/support-vector-machines-r     

## used to create a grid of red/black points     
x1range <- seq(-3, 4.2, by = 0.1); x2range <- seq(-2, 3, by = 0.1)
xgrid <- expand.grid(X1 = x1range, X2 = x2range)
ygrid <- predict(model, xgrid, decision.values = TRUE)

# extract svm's decision values for each point, which will be used to draw contour lines     
decision_vals = attributes(ygrid)$decision

plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2) # plot the grid of points colored by the decision     
points(df$X1, df$X2, col = df$y, pch = 19) # add the points in the original data     

## add some contour lines     
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0, add = TRUE, lwd = 2)
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0.5, add = TRUE, col = "black")
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = -0.5, add = TRUE, col = "red")
```

```{r}
model <- svm(y ~ . , data = df, scale = FALSE, kernel = "radial", gamma = 1, cost = 0.1)
```

```{r, echo = FALSE}
## used to create a grid of red/black points     
x1range <- seq(-3, 4.2, by = 0.1); x2range <- seq(-2, 3, by = 0.1)
xgrid <- expand.grid(X1 = x1range, X2 = x2range)
ygrid <- predict(model, xgrid, decision.values = TRUE)

# extract svm's decision values for each point, which will be used to draw contour lines     
decision_vals = attributes(ygrid)$decision

plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2) # plot the grid of points colored by the decision     
points(df$X1, df$X2, col = df$y, pch = 19) # add the points in the original data     

## add some contour lines     
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0, add = TRUE, lwd = 2)
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0.5, add = TRUE, col = "black")
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = -0.5, add = TRUE, col = "red")
```


```{r}
model <- svm(y ~ . , data = df, scale = FALSE, kernel = "radial", gamma = 1, cost = 10)
```

```{r, echo = FALSE}
## used to create a grid of red/black points     
x1range <- seq(-3, 4.2, by = 0.1); x2range <- seq(-2, 3, by = 0.1)
xgrid <- expand.grid(X1 = x1range, X2 = x2range)
ygrid <- predict(model, xgrid, decision.values = TRUE)

# extract svm's decision values for each point, which will be used to draw contour lines     
decision_vals = attributes(ygrid)$decision

plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2) # plot the grid of points colored by the decision     
points(df$X1, df$X2, col = df$y, pch = 19) # add the points in the original data     

## add some contour lines     
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0, add = TRUE, lwd = 2)
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0.5, add = TRUE, col = "black")
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = -0.5, add = TRUE, col = "red")

```


```{r}
model <- svm(y ~ . , data = df, scale = FALSE, kernel = "radial", gamma = 0.5, cost = 1)
```

```{r, echo = FALSE}
## used to create a grid of red/black points     
x1range <- seq(-3, 4.2, by = 0.1); x2range <- seq(-2, 3, by = 0.1)
xgrid <- expand.grid(X1 = x1range, X2 = x2range)
ygrid <- predict(model, xgrid, decision.values = TRUE)

# extract svm's decision values for each point, which will be used to draw contour lines     
decision_vals = attributes(ygrid)$decision

plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2) # plot the grid of points colored by the decision     
points(df$X1, df$X2, col = df$y, pch = 19) # add the points in the original data     

## add some contour lines     
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0, add = TRUE, lwd = 2)
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0.5, add = TRUE, col = "black")
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = -0.5, add = TRUE, col = "red")
```

```{r}
model <- svm(y ~ . , data = df, scale = FALSE, kernel = "radial", gamma = 0.5, cost = 0.10)
```

```{r, echo = FALSE}
## used to create a grid of red/black points     
x1range <- seq(-3, 4.2, by = 0.1); x2range <- seq(-2, 3, by = 0.1)
xgrid <- expand.grid(X1 = x1range, X2 = x2range)
ygrid <- predict(model, xgrid, decision.values = TRUE)

# extract svm's decision values for each point, which will be used to draw contour lines     
decision_vals = attributes(ygrid)$decision

plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2) # plot the grid of points colored by the decision     
points(df$X1, df$X2, col = df$y, pch = 19) # add the points in the original data     

## add some contour lines     
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0, add = TRUE, lwd = 2)
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0.5, add = TRUE, col = "black")
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = -0.5, add = TRUE, col = "red")
```


```{r}
model <- svm(y ~ . , data = df, scale = FALSE, kernel = "radial", gamma = 0.5, cost = 10)
```

```{r, echo = FALSE}
## used to create a grid of red/black points     
x1range <- seq(-3, 4.2, by = 0.1); x2range <- seq(-2, 3, by = 0.1)
xgrid <- expand.grid(X1 = x1range, X2 = x2range)
ygrid <- predict(model, xgrid, decision.values = TRUE)

# extract svm's decision values for each point, which will be used to draw contour lines     
decision_vals = attributes(ygrid)$decision

plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2) # plot the grid of points colored by the decision     
points(df$X1, df$X2, col = df$y, pch = 19) # add the points in the original data     

## add some contour lines     
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0, add = TRUE, lwd = 2)
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0.5, add = TRUE, col = "black")
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = -0.5, add = TRUE, col = "red")
```


```{r}
model <- svm(y ~ . , data = df, scale = FALSE, kernel = "radial", gamma = 5, cost = 1)
```

```{r, echo = FALSE}
## used to create a grid of red/black points     
x1range <- seq(-3, 4.2, by = 0.1); x2range <- seq(-2, 3, by = 0.1)
xgrid <- expand.grid(X1 = x1range, X2 = x2range)
ygrid <- predict(model, xgrid, decision.values = TRUE)

# extract svm's decision values for each point, which will be used to draw contour lines     
decision_vals = attributes(ygrid)$decision

plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2) # plot the grid of points colored by the decision     
points(df$X1, df$X2, col = df$y, pch = 19) # add the points in the original data     

## add some contour lines     
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0, add = TRUE, lwd = 2)
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0.5, add = TRUE, col = "black")
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = -0.5, add = TRUE, col = "red")
```


```{r}
model <- svm(y ~ . , data = df, scale = FALSE, kernel = "radial", gamma = 5, cost = 0.1)
```

```{r, echo = FALSE}
## used to create a grid of red/black points     
x1range <- seq(-3, 4.2, by = 0.1); x2range <- seq(-2, 3, by = 0.1)
xgrid <- expand.grid(X1 = x1range, X2 = x2range)
ygrid <- predict(model, xgrid, decision.values = TRUE)

# extract svm's decision values for each point, which will be used to draw contour lines     
decision_vals = attributes(ygrid)$decision

plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2) # plot the grid of points colored by the decision     
points(df$X1, df$X2, col = df$y, pch = 19) # add the points in the original data     

## add some contour lines     
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0, add = TRUE, lwd = 2)
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0.5, add = TRUE, col = "black")
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = -0.5, add = TRUE, col = "red")
```

```{r}
model <- svm(y ~ . , data = df, scale = FALSE, kernel = "radial", gamma = 5, cost = 10)
```

```{r, echo = FALSE}
## used to create a grid of red/black points     
x1range <- seq(-3, 4.2, by = 0.1); x2range <- seq(-2, 3, by = 0.1)
xgrid <- expand.grid(X1 = x1range, X2 = x2range)
ygrid <- predict(model, xgrid, decision.values = TRUE)

# extract svm's decision values for each point, which will be used to draw contour lines     
decision_vals = attributes(ygrid)$decision

plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2) # plot the grid of points colored by the decision     
points(df$X1, df$X2, col = df$y, pch = 19) # add the points in the original data     

## add some contour lines     
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0, add = TRUE, lwd = 2)
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = 0.5, add = TRUE, col = "black")
contour(x1range, x2range, matrix(decision_vals, nrow = length(x1range)), level = -0.5, add = TRUE, col = "red")
```

#### Write about the effect of the cost paramter:

Cost appears to be the relative importance of labeling individual data points correctly. As it increases, the contour lines tend to be closer together, and they tend to group more of the data of the same kind together.


#### Write about the effect of the gamma parameter:

Gamma appears to be the amount of complexity that the borders between groups can have. When it's low, the lines tend to be longer, sweeping curves, but as it increases, the lines show more "wiggle" as they bend back and forth much more quickly, changing concavity in a way that they won't when gamma is low. 



