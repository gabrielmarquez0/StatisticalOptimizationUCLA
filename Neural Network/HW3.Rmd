---
title: "Homework 3 - Neural Networks and Gradient Descent"
author: "Gabriel Marquez"
date: "Summer 2019"
output: html_document
---

Homework questions and text copyright Miles Chen. For personal use only. Do not distribute. Do not post or share your solutions.

For this homework assignment, we will build and train a simple neural network, using the famous "iris" dataset. We will take the four variables `Sepal.Length`, `Sepal.Width`, `Petal.Length`, and `Petal.Width` to create a prediction for the species.

We will train the network using gradient descent.

## Task 0:

Split the iris data into a training and testing dataset. Scale the data so the numeric variables are all between 0 and 1.

```{r}
library("NeuralNetTools")
# split between training and testing data
set.seed(1)
n <- dim(iris)[1]
rows <- sample(1:n, 0.8*n)
train <- iris[rows,]
test <- iris[-rows,]

# write your code here
sepal_l_max <- max(iris[,1])
sepal_w_max <- max(iris[,2])
petal_l_max <- max(iris[,3])
petal_w_max <- max(iris[,4])

train[,1] <- train[,1] / sepal_l_max
test[,1] <- test[,1] / sepal_l_max

train[,2] <- train[,2] / sepal_w_max
test[,2] <- test[,2] / sepal_w_max

train[,3] <- train[,3] / petal_l_max
test[,3] <- test[,3] / petal_l_max

train[,4] <- train[,4] / petal_w_max
test[,4] <- test[,4] / petal_w_max
```

## Setting up our network

Our neural network will have four neurons in the input layer - one for each numeric variable in the dataset. Our output layer will have three outputs - one for each species. There will be a `Setosa`, `Versicolor`, and `Virginica` node. When the neural network is provided 4 input values, it will produce an output where one of the output nodes has a value of 1, and the other two nodes have a value of 0. This is a similar classification strategy we used for the classification of handwriting digits.

I have arbitrarily chosen to have three nodes in our hidden layer.

We will add bias values before applying the activation function at each of our nodes in the hidden and output layers.

See slides 28-34 in Lecture 3-2.

## Task 1:

How many parameters are present in our model? List how many are present in: weight matrix 1, bias values for the hidden layer, weight matrix 2, and bias values for output layer.

#### Your answer: 

The weight matrix 1 has 12 parameters, four rows of inputs being the petal and sepal lengths and widths and three columns of outputs for the hidden layer values. The weight matrix 2 has 9 paramters, three rows of hidden layer inputs, and three columns of outputs for the floral categories. The bias values for the hidden layer have 3 values, one for each hidden layer node. The bias values for the output layer have 3 values, one for each floral category.

## Task 2: 

To express the categories correctly, we need to turn the factor labels in species column into vectors of 0s and 1s. For example, an iris of species _setosa_ should be expressed as `1 0 0`. Write some code that will do this. Hint: you can use `as.integer()` to turn a factor into numbers, and then use a bit of creativity to turn those values into vectors of 1s and 0s.

```{r}
# your code goes here

train_species <- matrix(0, 3, nrow = 120)
test_species <- matrix(0, 3, nrow = 30)
 
train_species[cbind(1:120,as.integer(train[,5]))] <- 1
test_species[cbind(1:30,as.integer(test[,5]))] <- 1

train <- cbind(train[1:4], train_species)
test <- cbind(test[1:4], test_species)
```


## Notation

We will define each matrix of values as follows:

$W^{(1)}$ the weights applied to the input layer.

$B^{(1)}$ are the bias values added before activation in the hidden layer.

$W^{(2)}$ the weights applied to the values coming from the hidden layer.

$B^{(2)}$ are the bias values added before the activation function in the output layer.

$J$ is a matrix of 1s so that the bias values in B can be added to all rows.


### Sigmoid Activation function

We will use the sigmoid function as our activation function. 

$$f(t) = \frac{1}{1 + e^{-t}}$$


## Forward Propagation

$$\underset{N\times 4}{\mathbf{X}} \underset{4 \times 3}{\mathbf{W}^{(1)}} + \underset{N\times 1}{\mathbf{J}}\underset{1\times 3}{\mathbf{B}^{(1)T}} = \underset{N\times 3}{\mathbf{z}^{(2)}}$$

$$f(\underset{N\times 3}{\mathbf{z}^{(2)}}) = \underset{N\times 3}{\mathbf{a}^{(2)}}$$

$$\underset{N\times 3}{\mathbf{a}^{(2)}} \underset{3 \times 3}{\mathbf{W}^{(2)}} + \underset{N\times 1}{\mathbf{J}}\underset{1\times 3}{\mathbf{B}^{(2)T}} = \underset{N\times 3}{\mathbf{z}^{(3)}}$$

$$f(\underset{N\times 3}{\mathbf{z}^{(3)}}) = \underset{N\times 3}{\mathbf{\hat y}}$$

## Easier Notation without bold face and without the dimensions underneath:

$$
XW^{(1)} + JB^{(1)T} = Z^{(2)}
$$

$$
f(Z^{(2)}) = A^{(2)}
$$

$$
A^{(2)}W^{(2)} + JB^{(2)T} = Z^{(3)}
$$

$$
f(Z^{(3)}) = \hat{Y}
$$

## Task 3: 

Express the forward propagation as R code using the training data. For now use random uniform values as temporary starting values for the weights and biases.

```{r}
# your code goes here
activation <- function(x) {
  return (1 / (1 + exp(-x)))
}

d_activation <- function(x) {
  return ((exp(-x))/((1 + exp(-x))^2))
}

X <- as.matrix(train[,1:4])

set.seed(3)
W1_3 <- matrix(runif(12, min = -0.5, max = 0.5), nrow = 4)
W2_3 <- matrix(runif(9, min = -0.5, max = 0.5), nrow = 3)

J <- matrix(1, nrow = 120)

B1_3 <- runif(3, min = -0.5, max = 0.5)
B2_3 <- runif(3, min = -0.5, max = 0.5)

Z2_3 <- X %*% W1_3 + J %*% B1_3
A2_3 <- activation(Z2_3)

Z3_3 <- A2_3 %*% W2_3 + J %*% B2_3
y_hat_3 <- activation(Z3_3)

```


## Back Propagation

The cost function that we will use to evaluate the performance of our neural network will be the squared error cost function:

$$J = 0.5 \sum (y - \hat{y})^2$$

```{r}
cost <- function(y,y_hat){
  0.5*sum((y - y_hat)^2)
}
```


## Task 4: (The hard task)

Find the gradient of the cost function with respect to the parameter matrices. 

You will create four partial derivatives, one for each of ($W^{(1)}, B^{(1)}, W^{(2)}, B^{(2)}$). 

This is known as back propagation. The value of the cost function ($J$) ultimately depends on the data ($X$) and our predictions ($\hat Y$). Our predictions ($\hat Y$) are just a result of a series of operations as seen above. Thus, when you calculate the derivative of the cost function, you will be applying the chain rule for derivatives as you take the derivative with respect to an early element.

Keep in mind that the derivative of $J$ with respect to a matrix will have the same dimensions as the matrix.

I recommend that you do the work on paper by hand, writing the dimensions underneath each component to make sure they align.

I hope the work done for you in Lectures 3-3 and 4-1 will be helpful as a guide to solving this. 

You do not need to show all of your work. You only need to typeset your final answers.

Feel free to work with each other, but don't post your full solutions to the derivatives on Campuswire.

### Your answer:

$$\frac{\partial J }{\partial W^{(2)}} = (\mathbf{a}^{(2)})^T \big(-(\mathbf{y} - \hat{\mathbf{y}})\odot f'(\mathbf{z}^{(3)})\big)$$

$$\frac{\partial J }{\partial B^{(2)}} = \mathbf{J}^T\big(-(\mathbf{y}-\hat{\mathbf{y}}) \odot f'(\mathbf{z}^{(3)})\big)$$

$$\frac{\partial J }{\partial W^{(1)}} = \mathbf{X}^T\bigg(-(\mathbf{y} - \hat{\mathbf{y}})\odot f'(\mathbf{z}^{(3)}) (\mathbf{W}^{(2)})^T \bigg) \odot f'(\mathbf{z}^{(2)})$$


$$\frac{\partial J }{\partial B^{(1)}} = \mathbf{J}^T\bigg(-(\mathbf{y}-\hat{\mathbf{y}}) \odot f'(\mathbf{z}^{(3)})(\mathbf{W}^{(2)})^T\bigg)\odot f'(\mathbf{z}^{(2)})$$

## Task 5: 

Turn your partial derivatives into R code. To make sure you have coded it correctly, it is always a good idea to perform numeric gradient checking, which will be your next task.

```{r}
# your code goes here
# delta3 = NULL # See the example handwriting_ann code. this could be useful

delta_3 <- as.matrix( -(train[,5:7] - y_hat_3) * d_activation(Z3_3) )

djdw2 <- t(A2_3) %*% delta_3

djdb2 = t(J) %*% delta_3

# delta2 = NULL

delta_2 <- as.matrix(delta_3 %*% t(W2_3) * d_activation(Z2_3))

djdw1 = t(X) %*% delta_2

djdb1 = t(J) %*% delta_2
```

## Numerical gradient checking

## Task 6:

Perform numeric gradient checking. For the purpose of this homework assignment, show your numeric gradient checking for just the $W^{(1)}$ matrix. You should do numeric gradient checking for all elements in your neural network, but for the sake of keeping the length of this assignment manageable, show your code and results for the first weight matrix only.

To perform numeric gradient checking, create an initial set of parameter values for all of the values (all weight matricies and all bias values). Calculate the predicted values based on these initial parameters, and calculate the cost associated with them. Store this 'initial' cost value.

You will then perturb one element in the $W^{(1)}$ matrix by a small value, say 1e-4. You will then recalculate the predicted values and associated cost. The difference between the new value of the cost function and the initial cost gives us an idea of the change in J. Divide that change by the size of the perturbation (1e-4), and we now have an idea of the slope (partial derivative). You'll repeat this for all of the elements in the $W^{(1)}$ matrix.

See slides 40-43 in lecture 2-2, and the example code in handwriting_ann.R

```{r}
# your code goes here

# create some initial arbitrary weights for the matrices W1, B1, W2, B2

set.seed(6)
W1_6 <- matrix(runif(12, min = -0.5, max = 0.5), nrow = 4)
W2_6 <- matrix(runif(9, min = -0.5, max = 0.5), nrow = 3)

B1_6 <- runif(3, min = -0.5, max = 0.5)
B2_6 <- runif(3, min = -0.5, max = 0.5)

# find the current value of the cost function for these weights

y_6 <- train[,5:7]

Z2_6 <- X %*% W1_6 + J %*% B1_6
A2_6 <- activation(Z2_6)

Z3_6 <- A2_6 %*% W2_6 + J %*% B2_6
y_hat_6 <- activation(Z3_6)

cost_6 <- cost(y_6, y_hat_6)

e <- 1e-4  # size of perturbation

# an empty placeholder to store our numeric gradient values
num_djdw1 <- matrix(0, nrow = 4 , ncol = 3) 

# a loop to perturb each value and store the change in cost function/size of perturbation
for(i in 1:12){  # calculate the numeric gradient for each value in the w1 matrix
  W1_6[i] <- W1_6[i] + e
  
  Z2_6 <- X %*% W1_6 + J %*% B1_6
  A2_6 <- activation(Z2_6)

  Z3_6 <- A2_6 %*% W2_6 + J %*% B2_6
  y_hat_6 <- activation(Z3_6)

  
  num_djdw1[i] <- cost_6 - cost(y_6, y_hat_6) # replace with appropriate value
  W1_6[i] <- W1_6[i] - e
}

```

Now check to make sure that the values produced by the numeric gradient check match the values of the gradient as calculated by the partial derivatives which you calculated in the previous task. The match won't be perfect, but should be pretty good.

```{r}
# your code goes here

delta_3 <- as.matrix( -(y_6 - y_hat_6) * d_activation(Z3_6) )

delta_2 <- as.matrix(delta_3 %*% t(W2_6) * d_activation(Z2_6))

djdw1 = t(X) %*% delta_2

```


## Gradient Descent

## Task 7:

We will now apply the gradient descent algorithm to train our network. This simply involves repeatedly taking steps in the direction opposite of the gradient. 

With each iteration, you will calculate the predictions based on the current values of the model parameters. You will also calculate the values of the gradient at the current values. Take a 'step' by subtracting a scalar multiple of the gradient. And repeat.

I will not specify what size scalar multiple you should use, or how many iterations need to be done. Just try things out. A simple way to see if your model is performing 'well' is to print out the predicted values of y-hat and see if they match closely to the actual values.

Consult the example code in 'handwriting_ann.R' and slide 26 in lecture 4-1.

```{r}
# your code goes here
X_7 <- as.matrix(train[,1:4])

J_7 <- matrix(1, nrow = 120)

set.seed(7)
W1_7 <- matrix(runif(12, min = -0.5, max = 0.5), nrow = 4)
W2_7 <- matrix(runif(9, min = -0.5, max = 0.5), nrow = 3)

B1_7 <- runif(3, min = -0.5, max = 0.5)
B2_7 <- runif(3, min = -0.5, max = 0.5)

Y_7 <- as.matrix(train[,5:7])

g <- 0.01
for(i in 1:3000) {
  
  Z2_7 <- X_7 %*% W1_7 + J_7 %*% B1_7
  A2_7 <- activation(Z2_7)

  Z3_7 <- A2_7 %*% W2_7 + J_7 %*% B2_7
  Y_hat_7 <- activation(Z3_7)
  
  delta_3_7 <- as.matrix( -(Y_7 - Y_hat_7) * d_activation(Z3_7) )

  djdw2_7 <- t(A2_7) %*% delta_3_7
  djdb2_7 = t(J_7) %*% delta_3_7

  delta_2_7 <- as.matrix(delta_3_7 %*% t(W2_7) * d_activation(Z2_7))

  djdw1_7 = t(X_7) %*% delta_2_7
  djdb1_7 = t(J_7) %*% delta_2_7
  
  W1_7 <- W1_7 - g * djdw1_7
  W2_7 <- W2_7 - g * djdw2_7
  B1_7 <- B1_7 - g * djdb1_7
  B2_7 <- B2_7 - g * djdb2_7
  
}


Z2_7 <- X_7 %*% W1_7 + J %*% B1_7
A2_7 <- activation(Z2_7)

Z3_7 <- A2_7 %*% W2_7 + J %*% B2_7
Y_hat_7 <- activation(Z3_7)

select <- function(x) {
  z <- numeric(length(x))
  z[x == max(x)] <- 1
  z
}
```


## Testing our trained model

Now that we have performed gradient descent and have effectively trained our model, it is time to test the performance of our network.

## Task 8:

Using the testing data, create predictions for the 30 observations in the test dataset. Print those results.

```{r}
# your code goes here
W1_8 <- W1_7
W2_8 <- W2_7

B1_8 <- B1_7
B2_8 <- B2_7

X_8 <- as.matrix(test[,1:4])

J_8 <- matrix(1, nrow = 30)

Z2_8 <- X_8 %*% W1_8 + J_8 %*% B1_8
A2_8 <- activation(Z2_8)

Z3_8 <- A2_8 %*% W2_8 + J_8 %*% B2_8
Y_hat_8 <- activation(Z3_8)


cbind(test, t(apply(Y_hat_8, 1, select)))

```

## Task 9:

Create a confusion matrix - a table that compares the predictions to the actual values. It will be a 3x3 table, and most values should be along the diagonal. Off-diagonal elements represent errors.

There is code to create a confusion matrix in 'handwriting_ann.R'

```{r}
table(as.data.frame(cbind("actual" = max.col(test[,5:7]), "results" = max.col(Y_hat_8))))

```


How many errors did your network make?


## Using package `nnet`

While instructive, the manual creation of a neural network is seldom done in production environments.

[Install the `nnet` and `NeuralNetTools` packages] Read the documentation for the function `nnet()`. I've created a neural network for predicting the iris species based on the four numeric variables. We use the same training data to train the network. The function `nnet()` is smart enough to recognize that the values in the species column are a factor and will need to expressed in 0s and 1s as we did in our manually created network.

```{r}
set.seed(1)
n <- dim(iris)[1]
rows <- sample(1:n, 0.8*n)
train <- iris[rows,]

library(nnet)
library(NeuralNetTools)
irismodel <- nnet(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, size=3, data = train)
```

Once we have created the network with nnet, we can use the predict function to make predictions for the test data.

```{r}
plotnet(irismodel) # a plot of our network

results <- max.col(predict(irismodel, iris[-rows,]))
results_df <- data.frame(results, actual = as.numeric(iris[-rows, 5]))
results_df
table(results_df)
# we can see that the predicted probability of each class matches the actual label
```




