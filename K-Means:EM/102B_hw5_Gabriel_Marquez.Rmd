---
title: "HW5: Clustering"
author: "Gabriel Marquez"
date: "September 13, 2019"
output: html_document
---

Stats 102B HW5. Copyright Miles Chen. Do not post or distribute without permission.

# Part 1: K-means Clustering

Read section 6.2 in the text and https://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm

K-means clustering is a clustering method. The algorithm can be described as follows:

0) Determine how many (k) clusters you will search for.
1) Randomly assign points in your data to each of the clusters.
2) Once all values have been assigned to a cluster, calculate the means or the centroid of the values in each cluster.
3) Reassign values to clusters by associating values in the data set to the nearest (Euclidean distance) centroid.
4) Repeat steps 2 and 3 until convergence. Convergence occurs when no values are reassigned to a new cluster.


```{r}
# Don't change this code. It will be used to generate the data.
set.seed(2019)
library(mvtnorm)
cv <- matrix(c(1,0,0,1), ncol=2)
j <- rmvnorm(100, mean = c(3,3), sigma = cv)
k <- rmvnorm(100, mean = c(5,8), sigma = cv)
l <- rmvnorm(100, mean = c(8,3), sigma = cv)
dat <- rbind(j,k,l)
true_groups <- as.factor(c(rep("j",100),rep("k",100),rep("l",100) ))
plot(dat, col=c('red','green', 'blue')[true_groups], asp = 1)
```

## Task 1

### Write code to perform k-means clustering on the values in the matrix `dat`.

The true group labels are provided in the vector `true_groups`. Of course, you can't use that until the very end where you will perform some verification.

Requirements:

1) So everyone will get consistent results, I have performed the initial assignment of points to clusters.
2) With each iteration, plot the data, colored by their current groupings, and the updated means.
3) Convergence is reached when group assignments no longer change. Your k-means clustering algorithm should reach convergence fairly quickly.
4) Print out a 'confusion' matrix showing how well the k-means clustering algorithm grouped the data vs the 'true labels.'

One suggestion is to write a function that will calculate the distances from a point to each of the three means. You can apply this function to the matrix of points (n x 2) and get back another matrix of distances (n x 3) where the columns are distance to centroid A, dist to centroid B, dist to centroid C.


```{r}
# do not modify this code
set.seed(2019)
assignments <- factor(sample(c(1,2,3), 300, replace = TRUE)) # initial groupings that you will need to update
plot(dat, col = c('red', 'green', 'blue')[assignments], asp = 1)  # initial plot
```

```{r}
# write your code here
distances <- function(point, means){
  # write code here
  # return(c(A = dist_to_centroid_A, B = dist_to_centroid_B, C = dist_to_centroid_C))
  c(((point[1] - means[1,1])^2 + (point[2] - means[1,2])^2)^.5, ((point[1] - means[2,1])^2 + (point[2] - means[2,2])^2)^.5, ((point[1] - means[3,1])^2 + (point[2] - means[3,2])^2)^.5 )
}

converged = FALSE
while(!converged){
  # write code here...
  prev_assignments <- assignments
  
  means <- matrix(c(apply(dat[assignments == 1,], 2, mean),
                    apply(dat[assignments == 2,], 2, mean),
                    apply(dat[assignments == 3,], 2, mean)),
                  byrow = TRUE,
                  nrow = 3)
  

  assignments <- apply(t(apply(dat, 1, function(x) {distances(x, means)})), 1, which.min)
  
  if(all(prev_assignments == assignments)) {
    converged <- TRUE
  }
  par(bg = rgb(.95,.95,.95,1))
  
  plot(dat, col=c('red','green', 'blue')[assignments], asp = 1)
  points(means, pch = 23, col= 'black', bg = c('red','green', 'blue')  )
}

table(as.data.frame(cbind("results" = assignments, "actual" = true_groups)))
```


# Part 2: Kernelized K-means Clustering

Read section 6.2.3 in the text.

### Our very simple data. 8 points. 4 near the origin. 4 that are spaced away

```{r}
X <- matrix(c(
  0.2,  0.2,
  0.2, -0.2,
 -0.2,  0.2,
 -0.2, -0.2,
    2,  2, 
    2, -2,
   -2, -2,
   -2,  2),
  byrow = TRUE,
  ncol = 2)
plot(X, asp = 1)
```

To perform Kernelized K-means clustering, we will use Kernel Functions. A Kernel transforms our 2D point into another point in a higher dimension and returns the inner (dot) product of pair of points in the higher dimension. (helpful to review what a dot product signifies: <https://youtu.be/LyGKycYT2v0> )

The function that tranforms the data from 2D to the higher dimension is called phi (\(\phi\left(\mathbf{x}\right)\)). For this example, Phi will transform our 2D data into 3D data.

We transform a 2D coordinate \(\phi\left(x_1, x_2\right)\) to the 3d coordinate: \(\left(x_1^2, x_2^2, \sqrt{2}x_1x_2\right)\) (equivalent to using a polynomial kernel with C = 0 and gamma = 2) (See: https://en.wikipedia.org/wiki/Polynomial_kernel)

```{r}
phi <- function(x){
  c(x[1]^2, x[2]^2, (2^.5)*x[1]*x[2] ) # change this
}

transformed <- t(apply(X, 1, FUN = phi))
transformed
```

## Task 2a: 

Instead of random assignments, place all points into the same cluster, with the exception of the first value, which will be placed in the second cluster. 

Using the coordinates of the data projected in 3 dimensions, calculate the (3-dimensional) centroids of each cluster.

```{r}

assignments_2 <- factor(c(1, rep(2, 7)))

means_2 <- matrix(c(transformed[assignments_2 == 1,],
                    apply(transformed[assignments_2 == 2,], 2, mean) ),
                  byrow = TRUE,
                  nrow = 2)

plot(X, col = assignments_2, asp = 1, pch = 19)

```

## Task 2b: 

Measure the squared Euclidean distance from each of the 8 values to both centroids. Produce a matrix that records these distances. It will be an 8 x 2 matrix. The first column will be the distance from each point to the first centroid (center of the 7 points in cluster 1). The second column will be the distances from each point to the second centroid (this cluster has only the first coordinate in it).

```{r}

euclid_distances <- function(point, means) {
  c(((point[1] - means[1,1])^2 + (point[2] - means[1,2])^2 + (point[3] - means[1,3])^2), ((point[1] -   means[2,1])^2 + (point[2] - means[2,2])^2 + (point[3] - means[2,3])^2))
}

distances_2 <- t(apply(transformed, 1, function(x) {euclid_distances(x, means_2)}))


```

## Task 2c:

Assign each point to the cluster whose centroid is nearer. Update your Z values accordingly.

```{r}
assignments_2 <- apply(distances_2, 1, which.min)

plot(X, col = assignments_2, asp = 1, pch = 19)

```

This takes us through just one iteration. Normally, we would iterate between the assignment step and the centroid calculation step. However, for this data set, one iteration is enough to properly cluster the points.

# Using Kernels

The use of a kernel function allows us to bypass the need to calculate the coordinates of each point in the higher-dimensional transformed space. We do not even need to know the function Phi. The Kernel function will simply return the dot product (a scalar value) of two points in the transformed space.

With some algebraic manipulation (show in the textbook), we can get the equation for the distances from each point to the cluster centroids in terms of the Kernel function, completely avoiding the use of phi.

## Task 3a:

To reduce computational time, we will create a matrix of all the products resulting from applying the Kernel function to every pair of points. (We find $K\left(\mathbf{x}_m, \mathbf{x}_r\right)$ for all pairs of m, r).  This is similar to creating a multiplication table, except we are using the Kernel function. The calculations in the future will use these Kernel function products, and rather than having to redo the calculation each time, we will simply look up the product in the table.

\[K\left(\mathbf{x}_m, \mathbf{x}_r\right) = \left(\mathbf{x}_m^T\mathbf{x}_r\right)^2\]

The Kernel matrix is equivalent to finding the inner products of the transformed data. However, do not use the transformed data, as the whole point of this part of the exercise is to show that we can achieve the same thing without transforming our data. Find the kernel results using the kernel function itself.

```{r}
kernel_3 <- apply(X, 1, function(x) {(x %*% t(X))^2})
```

## Task 3b: 

Use equation 6.3 from the textbook to calculate the kernelized distance matrix. The results of this should be equivalent to your results in task 2b. 

Note that the calculation of this distance matrix did not require the use of the transformation function phi at all.

```{r}
distances_3 <- matrix(rep(0,16), nrow = 8)

z <- matrix(c(1, rep(0,7),0, rep(1,7)), nrow = 8)

for(n in 1:8) {
  m_sum_1 <- 0
  m_sum_2 <- 0
  r_sum_1 <- 0
  r_sum_2 <- 0
  for(m in 1:8) {
    m_sum_1 <- m_sum_1 + z[m,1]* kernel_3[n,m]
    m_sum_2 <- m_sum_2 + z[m,2]* kernel_3[n,m]
    for(r in 1:8) {
      r_sum_1 <- r_sum_1 + z[m,1]*z[r,1]* kernel_3[m,r]
      r_sum_2 <- r_sum_2 + z[m,2]*z[r,2]* kernel_3[m,r]
    }
  }
  
  distances_3[n,1] <- kernel_3[n,n] - (2/1)*m_sum_1 + r_sum_1
  distances_3[n,2] <- kernel_3[n,n] - (2/7)*m_sum_2 + (1/49)*r_sum_2
}

cbind("phi_centroid_1" = distances_2[,1], "phi_centroid_2" = distances_2[,2],
      "kernel_centroid_1" = distances_3[,1], "kernel_centroid_2" = distances_3[,2])

```

## Task 3c: 

Assign each point to the cluster whose centroid is nearer. Update your Z matrix (the assignment matrix) accordingly.

```{r}

z <- matrix(rep(0,16), nrow = 8)

z[cbind(1:8,apply(distances_3, 1, which.min))] <- 1

z

plot(X, col = apply(distances_3, 1, which.min), asp = 1, pch = 19)

```

This takes us through just one iteration. Normally, we would iterate between the assignment step and the centroid calculation step. However, for this data set, one iteration is enough to properly cluster the points.

## More information about the Kernel

In this exercise, we used a simple 2D to 3D transformation function and its corresponding Kernel function. In real life, the most commonly used kernel function is the Radial Basis Function (RBF) Kernel, also called a Gaussian Kernel Function. <https://en.wikipedia.org/wiki/Radial_basis_function_kernel>

Like other kernel functions, the RBF kernel returns the inner product of a pair of points in a transformed space. The interesting thing is that the transformed space is infinite dimensional. It uses the (infinite) Taylor Expansion of the exponential function. <http://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/svms/RBFKernel.pdf>

Thus, getting the kernelized distances to the cluster centroids is only possible using the Kernel function. We would not actually be able to compute the transformation into infinite dimensional space.


# Part 3: EM Algorithm

The Expectation-Maximization algorithm is an iterative algorithm for finding maximum likelihood estimates of parameters when some of the data is missing. In our case, we are trying to estimate the model parameters (the means and sigma matrices) of a mixture of multi-variate Gaussian distributions, but we are missing the group information of the data points. That is, we do not know if a value belongs to group A, group B, or group C.

The general form of the EM algorithm consists of alternating between an expectation step (E) and a maximization step (M). 

In the expectation step, a function is calculated. The function is the expectation of the log-likelihood of the joint distribution of the data X along with the missing values of Z (cluster assignments) given the values of X under the current estimates of \(\theta\). (\(\theta\) is the umbrella parameter that encompasses the means and sigma matrices)

In the maximization step, the values of \(\theta\) are found that will maximize this expected log-likelihood.

We can take advantage of the fact that the solution to the maximization step can often be found analytically (versus having to search for it via a computational method.) For example, the estimate of the mean that maximizes the likelihood of the data is just the sample mean.

## EM Algorithm for Gaussian Mixtures

See EM Algorithm Notes Handout. Section 6.3.3 in the text also covers the EM algorithm but uses different notation.

This (brilliant) algorithm can be applied to perform clustering of Gaussian mixtures (among many other applications) in a manner similar to the k-means algorithm and Bayes classifier. A key difference between the k-means algorithm and the EM algorithm is that the EM algorithm is probabilistic. The k-means algorithm assigned a value to the group with the nearest mean. The EM algorithm calculates the probability that a point belongs to a certain group (much like the Bayes classifier).

In the context of a Gaussian mixture, we have the following components:

1) $X$ is our observed data
2) $Z$ is the missing data: the cluster to which the observations $X$ belong.
3) $X$ come from a normal distributions defined by the unknown parameters \(\Theta\) (the mean \(\mu\) and variance \(\Sigma\)). 
4) $Z$ is generated by a categorical distribution based on the unknown class mixing parameters \(\alpha\). (\(\sum \alpha_{i} = 1\))

Thus, 

\[P\left(x | \Theta\right) = \sum_{k = 1}^{K} \alpha_{k} P\left(X | Z_{k}, \theta_{k}\right)\]

We will use the following code to generate our data. It generates 1000 points.

```{r}
# Don't change this code. It will be used to generate the data.
set.seed(2019)
library(mvtnorm)
cv <- matrix(c(1,0,0,1), ncol=2)
j <- rmvnorm(200, mean = c(3,12), sigma = .5*cv)
k <- rmvnorm(600, mean = c(8,8), sigma = 4*cv)
l <- rmvnorm(200, mean = c(12,12), sigma = .5*cv)
dat <- rbind(j,k,l)
em_true_groups <- as.factor(c(rep("j",200),rep("k",600),rep("l",200) ))
plot(dat, main = "unlabeled data")
col = c("red", "blue", "green")
plot(dat, col = col[em_true_groups], main = "data with true group assignments")
```


The EM algorithm for Gaussian Mixtures will behave as follows:

1) Begin with some random or arbitrary starting values of \(\Theta\) and \(\alpha\).

2) E-Step. In the E-step, we will use Bayes' theorem to calculate the posterior probability that an observation $i$ belongs to component $k$.

\[w_{ik} = p\left(z_{ik} = 1 | x_{i}, \theta_{k}\right) = \frac{p\left(x_{i} | z_{k}, \theta_{k}\right) p\left(z_{k} = 1\right)}{\sum_{j = 1}^K p\left(x_{i} | z_{j}, \theta_{j}\right) p\left(z_{j} = 1\right)}\]

We will define \(\alpha_{k}\) as that the probability that an observation belongs to component $k$, that is \(p\left(z_{k} = 1\right) = \alpha_{k}\).

We also know that the probability of our $x$ observations follow a normal distribution. That is to say $p\left(x_{i} | z_{k}, \theta_{k}\right) = N\left(x_{i} | \mu_{j}, \Sigma_{j}\right)$. Thus, the above equation simplifies to:

\[w_{ik} = \frac{N\left(x_{i} | \mu_{k}, \Sigma_{k}\right) \alpha_{k}}{\sum_{j = 1}^K N\left(x_{i} | \mu_{j}, \Sigma_{j}\right) \alpha_{j}}\]

This is the expectation step. It essentially calculates the 'weight' or the 'responsibility' that component $k$ has for observation $i$. This reflects the expectations about the missing values of $z$ based on the current estimates of the distribution parameters \(\Theta\).

3) M-step. Based on the estimates of the 'weights' found in the E-step, we will now perform Maximum Likelihood estimation for the model parameters.

This turns out to be fairly straightforward, as the MLE estimates for a normal distribution are fairly easy to obtain analytically.

For each component, we will find the mean, variance, and mixing proportion based on the data points that are "assigned" to the component. The data points are not actually "assigned" to the components like they are in k-means, but rather the components are given a "weight" or "responsibility" for each observation.

Thus, our MLE estimates are:

\[\alpha_{k}^{new} = \frac{N_{k}}{N}\]

\[\mu_{k}^{new} = \frac{1}{N_{k}} \sum_{i = 1}^N w_{ik} x_{i}\]

\[\Sigma_{k}^{new} = \frac{1}{N_{k}} \sum_{i = 1}^N w_{ik} \left(x_{i} - \mu_{k}^{new}\right)\left(x_{i} - \mu_{k}^{new}\right)^T\]

4. Iterate between steps 2 and 3 until convergence is reached.

## Coding the EM algorithm for Gaussian Mixtures

Coding the algorithm is a matter of turning the above steps into code.

The package `mvtnorm` handles multivariate normal distributions. The function `dmvnorm()` can be used to find the probability of the data $N\left(x_{i} | \mu_{k}, \Sigma_{k}\right)$. It can even be applied in vector form, so you can avoid loops when trying to find the probabilities.

You are dealing with a 1000 x 2 matrix of data points.

A few key things to remember / help you troubleshoot your code:

1) Your matrix of 'weights' will be 1000 x 3. (one row for each observation, one column for each cluster)
2) $N_{k}$ is a vector of three elements. It is effectively the column sums of the weight matrix $w$.
3) \(\alpha\) is a vector of three elements. The elements will add to 1.
4) \(\mu\) is a 3 x 2 matrix. One row for each cluster, one column for each x variable.
5) Each covariance matrix sigma is a 2x2 matrix. There are three clusters, so there are three covariance matrices.

#### Tip for the covariance matrices \(\Sigma\)

As I was coding, I struggled a bit with creating the covariance matrices. I ended up having to implement the formula almost exactly as it was written. I wrote a loop to calculate each covariance matrix. My loop went through the data matrix, row by row. The operation \(\left(x_{i} - \mu_{k}^{new}\right)\left(x_{i} - \mu_{k}^{new}\right)^T\) taxes a 2x1 matrix and matrix-multiplies it by a 1x2 matrix, resulting in a 2x2 matrix. You need to do this for every row. Multiply the resulting 2x2 matrices by $w_{ik}$, and then add all of them together to form one 2x2 matrix. Then divide those values by $N_{k}$. That should give you \(\Sigma_{k}\) for one of the clusters.

#### Other tips

I also suggest running through your code one iteration at a time until you are pretty sure that it works. 

Another suggestion:

IMO, implementing the covariances is the hardest part of the code. Before trying to update the covariances, you can leave the covariance matrices as the identity matrix, or plug in the actual known covariance matrices for `sig1` `sig2` and `sig3`. This way you can test out the rest of the code to see if the values of the means are updating as you would expect.

## Output Requriements

1) Run your EM algorithm until convergence is reached. Convergence can be deemed achieved when the mu and/or sigma matrices no longer changes.

2) Print out the resulting estimates of $N_{k}$, the \(\mu\) and the \(\Sigma\) values.

3) Run the k-means clustering algorithm (not kernelized k-means) on the same data to estimate the clusters. (Your previous k-means code could work here, but you can also just use `kmeans()` which is probably faster.)

3) Produce three plots:

- Plot 1: Plot the original data, where the data is colored by the true groupings.
- Plot 2: Using the weight matrix, assign the data points to cluster that has the highest weight. Plot the data, colored by the estimated group membership.
- Plot 3: Using the results from the k-means clustering algorithm, plot the data colored by the k-means group membership.

```{r, fig.align = 'center'}

# use these initial arbitrary values
N <- dim(dat)[1]  # number of data points
alpha <- c(0.2,0.3,0.5)  # arbitrary starting mixing parameters
mu <- matrix(  # arbitrary means
    c(5,8,
      7,8,
      9,8),
    nrow = 3, byrow=TRUE
)
sig1 <- matrix(c(1,0,0,1), nrow=2)  # three arbitrary covariance matrices
sig2 <- matrix(c(1,0,0,1), nrow=2)
sig3 <- matrix(c(1,0,0,1), nrow=2)

## write your code here
weights <- matrix(rep(0,3000), nrow = 1000)

j <- 0

par(mfrow = c(3,4))
converged <- FALSE
while(!converged) {
  j <- j + 1
  
  prev_mu <- mu
  
  for(i in 1:1000) {
    weights[i,1] <- dmvnorm(dat[i,], mu[1,], sig1)
    weights[i,2] <- dmvnorm(dat[i,], mu[2,], sig2)
    weights[i,3] <- dmvnorm(dat[i,], mu[3,], sig3)
    weights[i,] <- weights[i,] * alpha
    weights[i,] <- weights[i,]/sum(weights[i,])
  }
  
  if(any(j == seq(1, 36, by = 3))) {
   plot(dat, col = rgb(weights))
   points(mu, pch = 23, col= 'black', bg = c('red','green','blue') )
  }
  
  alpha <- c(sum(weights[,1]), sum(weights[,2]), sum(weights[,3]))/N
  
  mu <-  1 / (N * alpha) * t(apply(weights, 2, function(x) {colSums(x * dat)}))
  
  sig1 <- matrix(rep(0,4), nrow = 2)
  sig2 <- matrix(rep(0,4), nrow = 2)
  sig3 <- matrix(rep(0,4), nrow = 2)
  sig4 <- matrix(rep(0,4), nrow = 2)
  
  for(i in 1:1000) {
    sig1 <- sig1 + weights[i,1] * (dat[i,] - mu[1,]) %*% t(dat[i,] - mu[1,])
    sig2 <- sig2 + weights[i,2] * (dat[i,] - mu[2,]) %*% t(dat[i,] - mu[2,])
    sig3 <- sig3 + weights[i,3] * (dat[i,] - mu[3,]) %*% t(dat[i,] - mu[3,])
  }
  sig1 <- 1 / (N * alpha[1]) * sig1
  sig2 <- 1 / (N * alpha[2]) * sig2
  sig3 <- 1 / (N * alpha[3]) * sig3
  
  if(all.equal(prev_mu, mu) == TRUE) {
    converged <- TRUE
  }
  
} # The graphs here are some of the interim steps. They're graphed by blending the colors based on the true weights rather than assigning the color to the max weight. Additionally, they are the more noticable transitions, so they're just from the first half of the iterations, and only every third of those, with the exception of the last graph, which is the final version.

alpha * N

mu

sig1

sig2

sig3

kmeans_4 <- kmeans(dat, 3)
kmeans_4[2]


```

### Plot 1: True Groupings
```{r}
plot(dat, col = col[em_true_groups], main = "data with true group assignments")
points(matrix(c(3,12,8,8,12,12), ncol = 2, byrow = TRUE), pch = 23, col= 'black', bg = c('red','blue','green') )
```

### Plot 2: EM Groupings
```{r}
plot(dat, col = rgb(weights == apply(weights,1,max)), main = "data with EM assignments")
points(mu, pch = 23, col= 'black', bg = c('red','green','blue') )
```


### Plot 3: K-Means Groupings
```{r}
plot(dat, col = col[kmeans_4[[1]]], main = "data with k-means assignment" )
points(kmeans_4[[2]], pch = 23, col= 'black', bg = c('red','blue','green'))
```

